{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f36947c-4ab7-4808-840b-93fbb80a302f",
   "metadata": {},
   "source": [
    "# Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c4ddc-85bb-4abe-abed-4c5dff800a80",
   "metadata": {},
   "source": [
    "- One to many: image captioning\n",
    "- Many to one: sequence of video frames -> action\n",
    "- Many to many: video captioning, ChatGPT\n",
    "\n",
    "- Many tasks require variable size inputs / outputs\n",
    "    - Example is language\n",
    "\n",
    "- Models reach human-level for language model benchmarks relatively quickly\n",
    "\n",
    "- Representing meaning of words\n",
    "    - In the past: mapping between idea -> image, definition, etc\n",
    "    - Wordnet: make hand-constructed synonyms & hypernyms (hierarchy of related objects)\n",
    "    - Imagenet classes were derived from Wordnet hierarchy\n",
    " \n",
    "    - Limitations of Wordnet\n",
    "        - Synonyms don't capture full nuance and their context\n",
    "        - Can't keep up with new words (slang, etc)\n",
    "        - Doesn't scale very well, need human labor to manually create these sets\n",
    "\n",
    "- Representing words\n",
    "    - Discrete representation\n",
    "        - Each word is a one-hot encoded vector\n",
    "        - Hard to represent word relationships: no notion of similarity between vectors\n",
    "\n",
    "    - Learning representation\n",
    "        - Use deep learning to represent words in vector space with notion of similarity\n",
    "        - Distributional semantics: word `w` is a function of its context\n",
    "\n",
    "        - Word vectors (word2vec)\n",
    "            - Real-valued vector that captures context of word\n",
    "            - Dot product shows similarity (similar to vectors)\n",
    "\n",
    "Word2Vec\n",
    "1. Collect corpus of text\n",
    "2. Pick most common V words from corpus (unknown words have `unk` token)\n",
    "3. Randomly initialize word vector\n",
    "4. Use \"center\" word to predict \"outside\" words near it. Maximize the probability of predicting the correct \"outside\" words.\n",
    "\n",
    "- Minimize loss = maximizing likelihood\n",
    "- To compute $P(w_{t+j} | w_t ; \\theta)$, use neural network\n",
    "\n",
    "- Model parameters ($\\theta$): $\\mathbb{R}^{2dV}$ (2 vectors for center and outside, d dims per word, V vocab size)\n",
    "    - Two vectors due to easier backprop calculation\n",
    "\n",
    "Skip-gram model\n",
    "- Get rid of $m$, $f$ predicts *all* words in document\n",
    "- Scales well, good for many tasks, but training is slow, not as good for word similarities\n",
    "\n",
    "Negative sampling\n",
    "- Calculating score is expensive since you need to iterate over entire vocabulary\n",
    "- Randomly sample $k$ words from vocabulary, use this as part of loss\n",
    "- Use heuristic to sample common words based on occurrence in vocabulary\n",
    "- Gradient matrix is sparse, so an implementation trick is embedding layer (only load vectors at a time)\n",
    "\n",
    "Co-occurence vector\n",
    "- Create V by V matrix with co-occurrence counts\n",
    "- Convert this into lower-dimensional, dense vector\n",
    "    - SVD: convert 500k x 500k to 500k x d\n",
    "        - This does not work well since common words (the, a, etc) have too much of an impact on vector\n",
    "        - There are ways to fix this\n",
    "- Representation gives interesting emergent behavior (linear transform of vector)\n",
    "- Mostly only good for similarities, SVD is slow, it's fast to train\n",
    "\n",
    "\n",
    "Log bilinear model\n",
    "- Each word has d dimensional vector\n",
    "- Use co-occurrence matrix with probability\n",
    "- Gives glove embeddings (good with analogies)\n",
    "\n",
    "Polysemy (words change meaning over time)\n",
    "- One vector per word is probably not good\n",
    "\n",
    "Problems with word vectors\n",
    "1. new words are hard to incorporate\n",
    "2. some words aren't frequent\n",
    "\n",
    "Word tokenization, subword tokenization (bytepair encoding)\n",
    "- Find commonly occurring n-grams and add it to the vocabulary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
