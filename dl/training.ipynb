{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b343b6-aa50-4247-93fb-b242868fa7c0",
   "metadata": {},
   "source": [
    "# Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6f6ef-cfb5-489b-8a3b-49e11d1e1f51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 1: Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9248af-7c7e-4901-b288-72b7f2a77a93",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b23e94-096e-49b0-8e63-c671e4b17c86",
   "metadata": {},
   "source": [
    "- **Naming conventions**: Neural networks are organized into layers: output of previous layer is connected to next layer.\n",
    "    - If you have $n$ layers, this doesn't count the input vector (but does count the final output layer).\n",
    "- **Output layer**: Output layer has no activation since it represents raw scores (softmax / svm loss will be applied to raw scores).\n",
    "- **Sizing**: count number of learning parameters in network.\n",
    "\n",
    "Universal approximators\n",
    "- Neural networks can approximate any real-valued function\n",
    "\n",
    "Choosing an architecture\n",
    "- **Larger models have more representational power**\n",
    "- **Prefer larger models**: don't use smaller models to avoid overfitting. Instead keep a larger model and use other techniques to fix overfitting: dropout, l2 regularization, input noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73b780-9fc2-403f-b50c-400c5627bd9a",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65edd620-cc9d-48af-99a3-482a332e760b",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "$\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "Pros\n",
    "- Squashes inputs to \\[0,1\\]\n",
    "\n",
    "Cons\n",
    "- Saturation: gradient tends to 0 at either end, so during backprop gradients may die\n",
    "- Outputs are not zero-centered: if the output of a layer is always positive, then in the *next* layer, all of its weights will update in the same direction (positive or negative), depending on the sign of the upstream gradient.\n",
    "    - This is because in the *next* layer, the gradient is $\\frac{d (Wx+b)}{d W} = x$, but $x>0$ (since the previous layer went through a sigmoid). So the gradient on the weights will be `local_grad * dout` = $x \\cdot \\texttt{dout}$.\n",
    "    - If you plot weight versus loss, the weights will only update in the same direction (hence zigzag pattern) and converge more slowly than if weights move in different, optimal directions towards minimum loss.\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "This is essentially sigmoid but zero-centered. Hence, it is always preferred to sigmoid.\n",
    "\n",
    "#### ReLU\n",
    "$f(x) = \\max(0,x)$\n",
    "\n",
    "Pros\n",
    "- No saturation\n",
    "- Efficient to compute\n",
    "- Converges quickly\n",
    "\n",
    "Cons\n",
    "- Not zero-centered\n",
    "- Dying ReLU: if inputs are negative, local grad will be zero\n",
    "    - To fix this, you can initialize bias to be positive (so that the activation will trigger)\n",
    "\n",
    "#### Leaky ReLU\n",
    "- Purpose is to fix dying ReLU problem\n",
    "- Has small positive slope when input is negative\n",
    "- Has all benefits of ReLU, but gradients will not die\n",
    "\n",
    "#### Maxout\n",
    "$\\max(w_1^T x + b_1, w_2^T x + b_2)$\n",
    "\n",
    "Generalizes ReLU and Leaky ReLU. Need more weights.\n",
    "\n",
    "### GeLU / ELU / SeLU / Swish\n",
    "GeLU: multiply input by 0 or 1 randomly. Useful for transformers.\n",
    "\n",
    "ELU is basically ReLU with closer-to zero-mean outputs. Downside is that you need to compute exp (more expensive than max).\n",
    "\n",
    "SeLU is a fancier version of ELU that self-normalizes.\n",
    "\n",
    "Swish: outperformed others in CIFAR-10.\n",
    "\n",
    "\n",
    "\n",
    "#### Summary\n",
    "In general: use ReLU. If you have dying ReLU: try leaky ReLU or maxout (for marginal gains). Try tanh as a last resort. Never use sigmoid.\n",
    "\n",
    "For transformers: use GeLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82823bf-3d8c-47ca-89e7-75dd8153e7d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 2: Preprocessing and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8bbc9-1054-4462-8dfe-2b7cf7a9e762",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Want: 1) zero-centered data with 2) unit variance.\n",
    "\n",
    "Compute mean only using training data (not validation/test sets), then apply preprocessing to train/val/test.\n",
    "\n",
    "Additional techniques\n",
    "- PCA: convert (N,D) to (N,100): pick out dimensions with highest variance\n",
    "- Whitening: achieves decorrelation (features are not correlated) and normalization (each feature has zero mean, unit variance)\n",
    "\n",
    "\n",
    "#### Weight initialization\n",
    "1. Constant weight initialization: bad because weights will learn exact same thing\n",
    "2. Random weight initialization: output variance may depend on neuron size\n",
    "    - For example, one neuron might output $w^T x + b$, so if $x$ has lots of elements, the output scalar has a larger variance\n",
    "3. Normalization: multiply by $\\frac{1}{\\sqrt{n}}$, where $n$ is the number inputs of the neuron.\n",
    "4. Kaiming / Xavier: multiply by $\\sqrt{\\frac{2}{n}}$\n",
    "    - When using ReLU especially, this ensures variance is normalized\n",
    "\n",
    "5. Batchnorm: normalization is differentiable. Insert a normalization layer *after* fully connected layer, but *before* nonlinearity. Normalization is done automatically by the network.\n",
    "\n",
    "    - Learnable parameters: $\\gamma, \\beta$: $x_{i,j} \\rightarrow \\gamma (x_{i,j} - \\mu) / \\sqrt{\\sigma} + \\beta$\n",
    "    - Batchnorm for FC nets: avg, stdev for each feature across all examples\n",
    "    - Batchnorm for convnets: avg, stdev for each channel (eg: rgb) across all pixels of all images\n",
    "    - Layernorm, instancenorm: just averaging over different values\n",
    "        - Layernorm: average with axis=1\n",
    "        - Instancenorm: each example and channel gets an average\n",
    "        - This'll make more sense in code (probably)\n",
    "\n",
    "#### Regularization\n",
    "1. L2: weights tend to small, diffuse numbers\n",
    "2. L1: weights tend to zero\n",
    "3. Max norm constraint: weights are capped at a constant $c$\n",
    "4. Dropout: keep neuron alive with probability $p$, dead otherwise.\n",
    "    - During prediction, hidden layer outputs must be scaled by probability $p$\n",
    "    - Inverted dropout: scale at train time to keep prediction fast\n",
    "\n",
    "Note: Don't usually regularize bias\n",
    "\n",
    "Overall, use dropout ($p=0.5$) with L2 regularization.\n",
    "\n",
    "\n",
    "### Loss Functions\n",
    "Various problem types:\n",
    "* Classification: discrete outputs\n",
    "    * Problem: lots of categories makes softmax layer slow. You can do fancy stuff with trees here: organize classes into a tree, and the output layer consists of decisions in the tree to go left or right.\n",
    "* Attribute classification: each input might belong to multiple classes (like an image might have many hashtags)\n",
    "    * Train a binary classifier for each attribute independently.\n",
    "    * Example: dog hashtag or not, cat hashtag or not, etc...\n",
    "* Regression: continuous outputs\n",
    "    * Find L2 norm between predicted value and actual value.\n",
    "    * Loss is much harder to optimize, since the network needs to be relatively close to the \"correct\" answer. For example, if desired output is 100, if the network outputs $1e4$, this will cause a huge gradient.\n",
    " \n",
    "    * Try to discretize problem before doing regression (it's fragile with L2 norm). For example, if you're predicting review stars from 1-5, do a classification instead of regression.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde027ca-2d29-4287-a31b-fbb2bab7b174",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 3: Learning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c75a8-8416-4e03-8de6-8fefa1b66d68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gradient Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158dfd60-c4d3-4243-97e3-b21d62100177",
   "metadata": {},
   "source": [
    "- Use centered formula: $\\frac{f(x+h)-f(x-h)}{2h}$\n",
    "- Use relative error: $\\frac{|a-b|}{\\max(a,b)}$\n",
    "- Use doubles instead of floats\n",
    "- Be careful of kinks: numerical gradient may disagree with analytical gradient. Analytical gradient may be zero, but numerical gradient might cross over the kink and give a slightly positive value.\n",
    "- Use a small number of datapoints to avoid kinks\n",
    "- Be careful of too-small $h$: might lead to instability\n",
    "- Do the check during a \"characteristic mode of operation\": doing gradcheck on initial weights might not be representative, so instead do gradcheck after running a few forward passes\n",
    "- Turn off randomization (dropout), or initialize a random seed. This is to ensure numerical and analytical computations are equivalent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57410c34-b856-45c7-91b3-df51d199c3dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcfb42d-1847-47b5-adcd-0ea0c370ebf9",
   "metadata": {},
   "source": [
    "- Ensure loss is the expected value with random weights\n",
    "- Increase regularization, see if it increases loss (it should)\n",
    "- Overfit on small set (20 examples). If this doesn't work, it's not worth training the whole network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071c6c1-c1e9-4b30-a874-dbb21bfc9892",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Babysitting Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81697469-5a82-46d1-9b58-76205b1ea30b",
   "metadata": {},
   "source": [
    "Mainly track *loss*, *accuracy*.\n",
    "\n",
    "#### Loss\n",
    "- Loss function should go down (ideally exponentially)\n",
    "- x-axis: epoch, y-axis: loss\n",
    "- What's an **epoch**?\n",
    "    - Training happens on *iterations* of *minibatches*. One epoch is when the entire training set has been processed (in expectation). So for example, if minibatch size is 5 and there's 20 total examples in training set, an epoch would consist of 4 minibatches.\n",
    " \n",
    "#### Accuracy\n",
    "- The closer validation acc is to training acc, the better generalization (and therefore less overfitting).\n",
    "- I think you can measure validation acc after each minibatch.\n",
    "\n",
    "#### Weight/Update Ratio\n",
    "- Measure good learning rate by finding ratio between weight and update.\n",
    "- Update / weight (per weight) should be around `1e-3`.\n",
    "\n",
    "#### Activation per Layer \n",
    "- Activations should cover whole range (ie: for tanh, it should be spread out between [-1,1]).s\n",
    "\n",
    "#### First-layer visualizations\n",
    "- For images, check first-layer weights: should be able to see smooth and defined features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713c6d6-319a-4b98-8767-116924cf5cc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01c6f4-71cb-4917-94bb-ab00a7bc83c0",
   "metadata": {},
   "source": [
    "#### Vanilla SGD:\n",
    "`W -= lr * dW`\n",
    "\n",
    "Issues with SGD:\n",
    "* **Slow convergence**: Gradient landscape might favor one direction over another\n",
    "* **Gets stuck**: Local minima or saddle points prevent finding the best weights\n",
    "    * Saddle points more common in multidimensional space\n",
    "    * Gradient is noisy approximation of optimal direction\n",
    "\n",
    "#### Momentum:\n",
    "```\n",
    "# accumulate past updates in velocity v (note decay factor mu)\n",
    "v  = mu*v - lr * dW\n",
    "W += v\n",
    "```\n",
    "\n",
    "* Keep updating even at local minima / saddle points with **momentum**\n",
    "* Momentum: keep track of past gradients to use as part of weight update\n",
    "\n",
    "#### Nesterov Momentum\n",
    "Compute gradient by looking ahead: instead of gradient at `W`, find gradient at `W+v*dW`. Has stronger theoretical guarantees for convex convergence.\n",
    "\n",
    "#### Anneal Learning Rate\n",
    "- Slow down learning rate over time\n",
    "- Cosine, linear, invsqrt, constant decay\n",
    "- Linear warmup: slowly warm up the learning rate, because a learning rate that is too high may lead to bad local minima\n",
    "\n",
    "#### Adagrad (adaptive gradient)\n",
    "- Each parameter has a learning rate\n",
    "- Keep a weighted average of previous gradients, use this for the update\n",
    "- Smaller weights have higher lr, vice versa\n",
    "\n",
    "#### RMSProp\n",
    "- With Adagrad, weights go to zero since gradient sum accumulates in the denominator\n",
    "- Decay the gradient so that weights don't go to zero\n",
    "\n",
    "#### Adam\n",
    "- Adam = RMSProp + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcb29a-8eb7-4d38-ac52-b023fec5ba43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe81be-7b15-46f6-a84d-14e3f86b9b3a",
   "metadata": {},
   "source": [
    "Train many models, average their predictions. May offer marginal gains.\n",
    "- Try different initializations, hyperparameters\n",
    "- Use checkpoints of model\n",
    "- Maintain copy of model in memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
