{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24382a27-659a-4f80-b730-c5ad8a37ca54",
   "metadata": {},
   "source": [
    "# Linear Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f679c65b-c559-495d-9b3a-693bf0919c81",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "From last time, kNNs have a few disadvantages:\n",
    "1. **Time Inefficient**: Test images must be compared to every single training image.\n",
    "2. **Space Inefficient**: *All* training data must be remembered for classification.\n",
    "\n",
    "A better approach consists of two functions:\n",
    "- **Score** function: maps image to class scores\n",
    "- **Loss** function: compares class scores to ground truth labels\n",
    "\n",
    "### Score function\n",
    "Assume $x_i \\in R^D$, and has label $y_i$. There's $N$ total training examples (so $i$ ranges from $1$ to $N$), and $K$ classes. We wish to define $f: R^D \\rightarrow R^K$ (a mapping from image to scores).\n",
    "\n",
    "In linear classifiers: $f(x_i, W, b) = Wx_i + b$, where $W$ is $(K \\times D)$, and b is $(K \\times 1)$. The rows of $W$ must be the same dimensions as the input image (remember the \"template\" analogy?). Each row of $W$ is multiplied by $x_i$ to produce the score for a class (dog, cat, etc).\n",
    "\n",
    "- Training data ($x_i$, $y_i$) is fixed. We're interested in changing $W$ and $b$.\n",
    "- Time efficient: single matrix multiply and add\n",
    "- Space efficient: don't have to store all training data\n",
    "\n",
    "#### Interpretations of linear classification\n",
    "1. Each row of $W$ expresses preferences for classification score.\n",
    "2. Think of images as points in 2d space. We adjust slope, y-intercept of weights to classify them.\n",
    "3. Each row of $W$ is a template for a class (dog, cat, etc). Image dot-product with a row produces score for a class.\n",
    "\n",
    "#### Bias trick, preprocessing\n",
    "- Bias trick: Add column to $W$ representing the biases. Add row (all $1$'s) to $x_i$.\n",
    "- Preprocessing: center data around $0$ (subtract $\\mu$), scale features to $[-1,1]$\n",
    "\n",
    "### Loss function\n",
    "Loss function is how unhappy we are with predictions on training set.\n",
    "\n",
    "#### Multiclass SVM loss\n",
    "$L_i = \\max(0, (s_j + \\Delta) - s_{y_i})$\n",
    "\n",
    "- Loss $L_i$ for image $x_i$ with label $y_i$\n",
    "- Multiclass SVM \"wants\" score of correct class to be higher than all other scores *by at least a margin of $\\Delta$*.\n",
    "    - If $s_{y_i}$ is bigger than $s_j + \\Delta$, then it contributes $0$ to loss. Score for correct class was sufficiently big enough, so no penalty in this case.\n",
    "    - If $s_{y_i}$ is smaller than $s_j + \\Delta$, then it contributes the difference (intuitively, as the score for the wrong class is bigger than the score for the correct class, the loss contributed increases).\n",
    "- L2-SVM loss: $L_i = \\max(0, \\cdot)^2$. Penalizes violated margins more strongly.\n",
    "\n",
    "#### Regularization\n",
    "- Problem: there are many different weights that result in the same loss (like $\\lambda W$). Which weights should we prefer?\n",
    "- We want loss function favor smaller weights: $L=(\\frac{1}{N} \\sum_i L_i) + \\lambda R(W)$ (choose $\\lambda$ experimentally)\n",
    "- L2 norm regularization: $R(W) = \\sum_k \\sum_l W_{k,l}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488d775-5169-4846-acc2-4ee629ed91be",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Hinge loss becomes cross-entropy loss, which is defined as:\n",
    "\n",
    "$L_i = - \\log(\\frac{e^{f_{y_i}}}{\\sum_{j} e^{f_j}})$\n",
    "\n",
    "Basically, this is applying negative log to the probability for the correct class. Note that this uses softmax function: $f_j(z) = \\frac{e^{z_j}}{\\sum_{k}{e^{z_k}}}$, where $z$ is a vector. There are 2 important properties: vector sums to 1, and each value is between 0 and 1.\n",
    "\n",
    "### Simple example\n",
    "Consider applying softmax to score vector:\n",
    "$f(Wx) = \n",
    "f(\\begin{bmatrix}\n",
    "4\\\\\n",
    "5\\\\\n",
    "6\\\\\n",
    "\\end{bmatrix})\n",
    "= \\begin{bmatrix}\n",
    "\\frac{e^{4}}{e^4 + e^5 + e^6}\\\\\n",
    "\\frac{e^{5}}{e^4 + e^5 + e^6}\\\\\n",
    "\\frac{e^{6}}{e^4 + e^5 + e^6}\\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0.10\\\\\n",
    "0.24\\\\\n",
    "0.66\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Numeric computation\n",
    "Computing softmax can lead to exponential blowup because the $\\exp$ terms might get really big. So instead we use an equivalent form:\n",
    "$f_j(z) = \\frac{e^{z_j}}{\\sum_{k}{e^{z_k}}} = \\frac{e^{z_j - C}}{\\sum_{k}{e^{z_k - C}}}$, where $C$ is the max value of the vector $z$. As a result, the exponent is at most zero (it could also be negative, but that's ok because $\\exp$ of a negative is small)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
