{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7cac4c-7a93-4d43-84a4-7d1a4462b3d3",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01da82-d02d-42ad-a6c3-d433dbbbc002",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a5004-1ed5-4265-b24e-c30d317ef92e",
   "metadata": {},
   "source": [
    "Components:\n",
    "1. Score function\n",
    "2. Loss function\n",
    "3. **Optimization**\n",
    "\n",
    "1 varies for different architectures (NNs, CNNs, etc). 2 and 3 mostly stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6789edc-8a80-4758-a43c-fc78f10a3ab8",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10568f0d-d134-4111-a640-9dfd0a5c9793",
   "metadata": {},
   "source": [
    "- $L$ is bowl-shaped. We are trying to reach the bottom by adjusting $W$.\n",
    "- SVM loss is piecewise-linear, and convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512df9aa-0448-4ae0-af0d-d41f47e8e499",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228718fc-8a34-4bb7-bcf7-05afaaf1746b",
   "metadata": {},
   "source": [
    "1. Random search: randomize $W$, then compute loss. Repeat a bunch of times. Pick $W$ that resulted in the smallest loss.\n",
    "2. Random local search: randomize $\\delta W$ (perturbation), compute loss with $W+\\delta W$. If loss decreased, use $W+\\delta W$ as the new weights.\n",
    "3. Follow gradient: instead of $\\delta W$ (random perturbation), we step in a direction that's guaranteed to decrease loss. This is done with $\\frac{dL}{dW}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92474460-36c6-4e34-bfdb-721b9d8eb58f",
   "metadata": {},
   "source": [
    "### Numerical Gradient\n",
    "Numerical gradient: $\\frac{f(x+h)-f(x)}{h}$.\n",
    "- Note: $f$ outputs a scalar, $x$ is a matrix or vector.\n",
    "- To compute this, nudge each dimension of $x$ by $h$, and see its (scalar) effect on $f$.\n",
    "- Resulting gradient matrix (or vector) is same shape as $x$: one gradient entry for each variable in $x$.\n",
    "\n",
    "Then, update weights with $W_{\\text{new}} = W - \\alpha \\frac{dL}{dW}$.\n",
    "\n",
    "#### Advantages:\n",
    "- Easy to implement\n",
    "\n",
    "#### Disadvantages:\n",
    "- Approximate: $h$ could always be arbitrarily smaller\n",
    "- Slow to compute: need to recompute loss for each entry of $W$ (compute over entire dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f8374-7372-4611-ba03-c5d1b7838424",
   "metadata": {},
   "source": [
    "### Analytical Gradient\n",
    "\n",
    "- Exact, faster, but harder to implement (since we might mess up gradient equations)\n",
    "- Gradient check: use numerical gradient as sanity check\n",
    "- Example: SVM gradient\n",
    "\n",
    "    - $\\nabla_{w_{y_i}} L_i = -( \\sum_{j \\neq y_i} \\mathbb{1}[w_j^T x_i + \\Delta - w_{y_i}^Tx_i > 0]) x_i$\n",
    "    - $\\nabla_{w_j} L_i = \\mathbb{1} (w_j^Tx_i+\\Delta-w_{y_i}^Tx_i > 0)x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa4ece-1c35-4dd7-a918-6cc4b7528cd0",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852291c-94cd-42f8-b7b8-e7aa17b9e384",
   "metadata": {},
   "source": [
    "```\n",
    "weights_grad = eval_grad(loss_fun, data, weights)\n",
    "weights -= step_size * weights_grad\n",
    "```\n",
    "\n",
    "### Minibatch Gradient Descent\n",
    "- Gradient descent requires computing over entire $(x,y)$ to update parameters once.\n",
    "- Idea: gradient descent on smaller batches, so we update parameters more often and model converges quicker\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "- Special case of minibatch gd, where batch size is one. Not common since vectorization is fast."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
